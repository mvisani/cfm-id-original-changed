/*#########################################################################
# Mass Spec Prediction and Identification of Metabolites
#
# nn_param.cpp
#
# Description: 	Class for parameterization of fragmentation probabilities
#				within the bayesian network fragmentation trees.
#
# Copyright (c) 2013, Felicity Allen
# Copyright (c) 2016, Fei Wang
# All rights reserved.

# This file is part of the cfm-id project.
# The contents are covered by the terms of the GNU Lesser General Public
# License, which is included in the file license.txt, found at the root
# of the cfm source tree.
#########################################################################*/

#include "NNParam.h"
#include "Config.h"

NNParam::NNParam(std::vector<std::string> a_feature_list, int a_num_energy_levels,
                 std::vector<int> &a_hlayer_num_nodes, std::vector<int> &a_act_func_ids,
                 std::vector<float> &a_dropout_probs, boost::container::vector<bool> & a_is_frozen):
        h_layer_num_nodes(a_hlayer_num_nodes), act_func_ids(a_act_func_ids),
        hlayer_dropout_probs(a_dropout_probs), hlayer_is_frozen(a_is_frozen),
        Param(a_feature_list, a_num_energy_levels) {

    //The weight length was set in the Param constructor, now alter it to match the neural net layout
    unsigned int num_features = weights.size() / num_energy_levels;
    input_layer_node_num = num_features;

    // set input layer
    int total_len = 0, num_input;
    auto hlayer_node_num = h_layer_num_nodes.begin();
    int num_input_layer_weights = (*hlayer_node_num) * num_features;
    total_len += num_input_layer_weights;//The first layer takes the fv as input (which has in-built bias).
    num_weights_per_layer.push_back(num_input_layer_weights);

    // handle hidden layers
    total_nodes = (*hlayer_node_num);
    num_input = (*hlayer_node_num);
    ++hlayer_node_num;

    for (; hlayer_node_num != h_layer_num_nodes.end(); ++hlayer_node_num) {
        int num_weights = (*hlayer_node_num) * (num_input + 1);
        num_weights_per_layer.push_back(num_weights);

        total_len += num_weights;
        total_nodes += *hlayer_node_num;
        num_input = *hlayer_node_num;
    }
    weights.resize(total_len * num_energy_levels);

    //Set pointers to the activation functions and their derivatives used in each layer
    setActivationFunctionsFromIds();

    //Resize some temporary a and z vectors so we don't have to
    // allocate memory for a and z within the computeTheta function if we don't need them after.
    tmp_z_values.resize(total_nodes);
    tmp_a_values.resize(total_nodes);

    //Roll Drop outs
    //last dropped out for output node
    //add this one for not to break stuff
    while (hlayer_dropout_probs.size() < h_layer_num_nodes.size())
        hlayer_dropout_probs.push_back(0);
    rollDropouts();

    //fill frozen flags if not provided
    while (hlayer_is_frozen.size() < h_layer_num_nodes.size())
        hlayer_is_frozen.push_back(false);
}

void NNParam::initWeights(int init_type) {
    switch (init_type) {
        case PARAM_FULL_ZERO_INIT:
            fullZeroInit();
            break;
        case PARAM_ZERO_INIT:
            zeroInit();
            break;
        case PARAM_NORMAL_INIT:
            randomNormalInit();
            break;
        case NN_PARAM_VAR_SCALING_INIT:
            varianceScalingInit();
            break;
        case PARAM_RANDOM_INIT:
        default:
            randomUniformInit();
    }
}

//Randomly initialise all weights
void NNParam::randomUniformInit() {
    float min = -0.1, max = 0.1;
    // All Terms: to uniform values between -0.1 and 0.1 - Biases too
    std::uniform_real_distribution<float> distribution(min, max);
    for (unsigned int i = 0; i < weights.size(); i++)
        weights[i] = distribution(util_rng);//(float(std::rand()) / float(RAND_MAX) - 0.5) * 0.2;
}

void NNParam::randomNormalInit() {
    // All Terms: to normal values in mean and std
    float mean = 0.0f, std_dev = 0.05f, min = -0.1f, max = 0.1f;
    std::normal_distribution<float> distribution(mean, std_dev);

    unsigned int energy_length = getNumWeightsPerEnergyLevel();
    for (unsigned int energy_level_idx = 0; energy_level_idx < getNumEnergyLevels(); energy_level_idx++) {
        int weight_offset = 0;
        for (const auto &num_weights : num_weights_per_layer) {
            for (unsigned int i = 0; i < num_weights; i++) {
                float weight = 0;
                do {
                    weight = distribution(util_rng);
                } while ((weight < min) || (weight > max));
                weights[energy_length * energy_level_idx + weight_offset + i] = weight;
            }
            weight_offset += num_weights;
        }
    }
}

// Variance Scaling Initializer
void NNParam::varianceScalingInit() {
    float factor = 1.0f;
    float mean = -0.0f;

    // All Terms: to normal values in mean and std
    unsigned int energy_length = getNumWeightsPerEnergyLevel();
    for (unsigned int energy_level_idx = 0; energy_level_idx < getNumEnergyLevels(); energy_level_idx++) {
        int weight_offset = 0;
        for (int hlayer_idx = 0; hlayer_idx < h_layer_num_nodes.size(); ++hlayer_idx) {
            int fan_in = input_layer_node_num;
            if (hlayer_idx > 0)
                fan_in = h_layer_num_nodes[hlayer_idx - 1];
            int fan_out = h_layer_num_nodes[hlayer_idx];
            int num_weights = num_weights_per_layer[hlayer_idx];

            float std_dev = sqrt(factor / float(fan_out + fan_in));
            float min = -2 * std_dev, max = 2 * std_dev;
            std::normal_distribution<float> distribution(mean, std_dev);

            for (unsigned int i = 0; i < num_weights; i++) {
                float weight = 0;
                do {
                    weight = distribution(util_rng);
                } while ((weight < min) || (weight > max));
                weights[energy_length * energy_level_idx + weight_offset + i] = weight;
            }
            weight_offset += num_weights;
        }
    }
}

float NNParam::computeTheta(const FeatureVector &fv, int energy) {
    return computeTheta(fv, energy, tmp_z_values, tmp_a_values, true, false);
}

float NNParam::computeTheta(const FeatureVector &fv, int energy, azd_vals_t &z_values, azd_vals_t &a_values,
                             bool already_sized, bool use_dropout) {

    //Check Feature Length
    int fv_length = fv.getTotalLength();
    if( fv_length != expected_num_input_features ){
        std::cout << "Expecting feature vector of length " << expected_num_input_features;
        std::cout << " but found " << fv_length << std::endl;
        throw( ParamFeatureMismatchException() );
    }
    int energy_offset = getNumWeightsPerEnergyLevel() * energy;
    auto weights_it = weights.begin() + energy_offset;
    //Resize the z and a vectors to the required sizes
    if( !already_sized) {
        z_values.resize( total_nodes );
        a_values.resize( total_nodes );
    }

    //The first hidden layer takes the fv as input (which already has a bias feature,
    // so no additional biases in this layer)
    int neuron_idx = 0;
    auto layer_it = h_layer_num_nodes.begin();

    for (int h_node = 0; h_node < (*layer_it); h_node++) {
        if (!use_dropout || !hlayer_is_dropped[neuron_idx]) {
            float z_val = 0.0f;
            // for each feature, note feature_it is the index of feature
            // since we are using a sparse binary feature vector
            // each value is the index of value one
            for (auto feature_it = fv.getFeatureBegin(); feature_it != fv.getFeatureEnd(); ++feature_it)
                z_val += *(weights_it + *feature_it);
            weights_it += fv_length;
            // set input value z and active value a
            z_values[neuron_idx] = z_val;
            a_values[neuron_idx] = act_funcs[neuron_idx](z_val);
            // update act value if we are using drop out
            if (use_dropout)
                a_values[neuron_idx] = a_values[neuron_idx]/(1.0 - hlayer_dropout_probs[0]);

        } else if (hlayer_is_dropped[neuron_idx]){
            // if we are using drop out
            // and current node is dropped
            weights_it += fv_length;
            z_values[neuron_idx] = 0.0f;
            a_values[neuron_idx] = 0.0f;
        }
        neuron_idx++;
    }

    //Subsequent layers take the previous layer as input
    int num_input = h_layer_num_nodes[0];
    int input_layer_node_idx_start = 0;
    for (int h_layer_idx = 1; h_layer_idx < h_layer_num_nodes.size(); ++h_layer_idx) {
        for (int h_node_idx = 0; h_node_idx < h_layer_num_nodes[h_layer_idx]; ++h_node_idx) {
            if (!use_dropout || !hlayer_is_dropped[neuron_idx]) {
                float z_val = *weights_it++; //Bias

                // sum up and record z_val
                for (int i = 0; i < num_input; i++)
                    z_val += a_values[input_layer_node_idx_start + i] * (*weights_it++);

                // set input value z and active value a
                z_values[neuron_idx] = z_val;
                a_values[neuron_idx] = act_funcs[neuron_idx](z_val);

                // update output value if we are using drop out
                if (use_dropout)
                    a_values[neuron_idx] = a_values[neuron_idx]/(1.0 - hlayer_dropout_probs[h_layer_idx]);

            } else if (hlayer_is_dropped[neuron_idx]){
                // if dropped out move by num_input + 1
                // 1 for bais num_input weights
                weights_it += 1 + num_input;
                z_values[neuron_idx] = 0.0f;
                a_values[neuron_idx] = 0.0f;
            }
            neuron_idx ++;
        }

        input_layer_node_idx_start += num_input;
        num_input = h_layer_num_nodes[h_layer_idx];
    }

    return a_values.back();    //The output of the last layer is theta
}

void NNParam::saveToFile(std::string &filename) {

    //Write all the weights etc using the parent function
    Param::saveToFile(filename);

    //Re-open the file and add the neural net configuration parameters
    std::ofstream out;
    out.open(filename.c_str(), std::fstream::app);
    if (!out.is_open()) {
        std::cout << "Warning: Trouble opening parameter file" << std::endl;
    } else {
        out << "Neural Net Config" << std::endl;
        out << h_layer_num_nodes.size() << std::endl;
        std::vector<int>::iterator iit = h_layer_num_nodes.begin();
        for (; iit != h_layer_num_nodes.end(); ++iit)
            out << *iit << " ";
        out << std::endl;
        out << act_func_ids.size() << std::endl;
        iit = act_func_ids.begin();
        // output activation function for each layer
        for (; iit != act_func_ids.end(); ++iit)
            out << *iit << " ";
        out << std::endl;
        // output dropout probs for each layer
        for (const auto &prob : hlayer_dropout_probs)
            out << prob << " ";
        out << std::endl;
        // output frozen flag for each layer
        for (const auto &is_frozen : hlayer_is_frozen)
            out << is_frozen << " ";
        out << std::endl;

        out.close();
    }
}

NNParam::NNParam(std::string &filename) : Param(filename) {

    //We've already read all the weights, here we want to read the neural net parameters,
    //which should be appended right at the end of the file
    std::string line;
    std::ifstream ifs(filename.c_str(), std::ifstream::in);

    bool found_nn_details = false;
    if (!ifs) std::cout << "Could not open file " << filename << std::endl;
    while (getline(ifs, line)) {
        if (line.size() > 6 && line.substr(0, 6) == "Neural") {
            //Get the hidden node number configuration
            getline(ifs, line);
            total_nodes = 0;
            int num_layers = std::stoi(line);
            h_layer_num_nodes.resize(num_layers);
            getline(ifs, line);
            std::stringstream ss1(line);
            for (int i = 0; i < num_layers; i++) {
                ss1 >> h_layer_num_nodes[i];
                total_nodes += h_layer_num_nodes[i];
            }
            //Get the activation function configuration
            getline(ifs, line);
            int act_func_size = std::stoi(line);
            act_func_ids.resize(act_func_size);
            getline(ifs, line);
            std::stringstream ss2(line);
            for (int i = 0; i < num_layers; i++)
                ss2 >> act_func_ids[i];
            setActivationFunctionsFromIds();

            // Get the dropout probs for each layer
            hlayer_dropout_probs.resize(act_func_size);
            getline(ifs, line);
            if(!ifs.eof()){
                std::stringstream ss3(line);
                // get  and roll drop outs
                for (int i = 0; i < num_layers; i++)
                    ss3 >> hlayer_dropout_probs[i];
            }
            rollDropouts();

            // Get the frozen flag for each layer
            hlayer_is_frozen.resize(act_func_size);
            getline(ifs, line);
            if(!ifs.eof()){
                std::stringstream ss4(line);
                // get  and roll drop outs
                for (int i = 0; i < num_layers; ++i)
                    ss4 >> hlayer_is_frozen[i];
            }

            tmp_z_values.resize(total_nodes);
            tmp_a_values.resize(total_nodes);
            found_nn_details = true;
            break;
        }
    }
    ifs.close();

    if (!found_nn_details) throw NNParamFileReadException();
}

void NNParam::setActivationFunctionsFromIds() {

    std::vector<int>::iterator it, itt;
    itt = h_layer_num_nodes.begin();
    for (it = act_func_ids.begin(); it != act_func_ids.end(); ++it, ++itt) {
        for (int hnode = 0; hnode < *itt; hnode++) {
            if (*it == LINEAR_NN_ACTIVATION_FUNCTION) {
                act_funcs.push_back(linear_activation);
                deriv_funcs.push_back(linear_derivative);
            } else if (*it == RELU_AND_NEG_RLEU_NN_ACTIVATION_FUNCTION) {
                if (hnode % 2 == 1) {
                    act_funcs.push_back(neg_relu_activation);
                    deriv_funcs.push_back(neg_relu_derivative);
                } else {
                    act_funcs.push_back(relu_activation);
                    deriv_funcs.push_back(relu_derivative);
                }
            } else if (*it == LEAKY_RELU_NN_ACTIVATION_FUNCTION) {
                act_funcs.push_back(leaky_relu_activation);
                deriv_funcs.push_back(leaky_relu_derivative);
            } else if (*it == RELU_NN_ACTIVATION_FUNCTION) {
                act_funcs.push_back(relu_activation);
                deriv_funcs.push_back(relu_derivative);
            } else throw NNParamActivationFunctionIdException();
        }
    }

}

void NNParam::computeDeltas(std::vector<azd_vals_t> &deltasA, std::vector<azd_vals_t> &deltasB,
                            std::vector<azd_vals_t> &z_values, std::vector<azd_vals_t> &a_values, float rho_denom,
                            int energy) {

    //Resize the output delta vectors
    unsigned int num_trans_from_id = z_values.size();
    deltasA.resize(num_trans_from_id);
    deltasB.resize(num_trans_from_id);
    for (int idx = 0; idx < num_trans_from_id; idx++) {
        deltasA[idx].resize(total_nodes);
        deltasB[idx].resize(total_nodes);
    }

    //Set up the iterators (we are going backwards through the network, so set up iterators in reverse)
    std::vector<azd_vals_t::reverse_iterator> itAs(num_trans_from_id), itBs(num_trans_from_id);
    std::vector<azd_vals_t::reverse_iterator> aits(num_trans_from_id), zits(num_trans_from_id);
    for (int idx = 0; idx < num_trans_from_id; idx++) {
        aits[idx] = a_values[idx].rbegin();
        zits[idx] = z_values[idx].rbegin();
        itAs[idx] = deltasA[idx].rbegin();
        itBs[idx] = deltasB[idx].rbegin();
    }
    std::vector<float (*)(float)>::reverse_iterator itdf = deriv_funcs.rbegin();
    std::vector<int>::reverse_iterator itlayer = h_layer_num_nodes.rbegin();
    std::vector<azd_vals_t::reverse_iterator> itAs_input = itAs, itBs_input = itBs;

    int energy_offset = (num_energy_levels - energy - 1) * getNumWeightsPerEnergyLevel();
    std::vector<float>::reverse_iterator wit = weights.rbegin() + energy_offset;

    //Compute the highest level deltas
    for (int idx = num_trans_from_id - 1; idx >= 0; idx--) {
        float deriv_val = (*itdf)(*(zits[idx])++);
        float theta = (*(aits[idx])++);
        float rho_val = exp(theta) / rho_denom;
        *(itAs[idx])++ = deriv_val;
        *(itBs[idx])++ = deriv_val * rho_val;
    }
    itlayer++;
    itdf++;
    int num_output = 1;

    //Compute the deltas for the subsequent layers
    std::vector<float>::reverse_iterator itA_input_tmp, itB_input_tmp, wit_tmp;
    for (; itlayer != h_layer_num_nodes.rend(); ++itlayer) {
        std::vector<float (*)(float)>::reverse_iterator itdf_tmp;
        for (int idx = num_trans_from_id - 1; idx >= 0; idx--) {
            itdf_tmp = itdf;
            for (int hnode_r = 0; hnode_r < *itlayer; hnode_r++) {
                wit_tmp = wit + hnode_r;
                itA_input_tmp = itAs_input[idx];
                itB_input_tmp = itBs_input[idx];
                float Aval = 0.0, Bval = 0.0;
                float deriv_val = (*itdf_tmp)(*zits[idx]++);
                for (int oidx = num_output - 1; oidx >= 0; oidx--) {
                    float w_deriv_val = (*wit_tmp) * deriv_val;
                    Aval += (*itA_input_tmp++) * w_deriv_val;
                    Bval += (*itB_input_tmp++) * w_deriv_val;
                    wit_tmp += (oidx > 0) * (*itlayer + 1);
                }
                *(itAs[idx])++ = Aval;
                *(itBs[idx])++ = Bval;
                itdf_tmp++;
            }
            itAs_input[idx] = itA_input_tmp;
            itBs_input[idx] = itB_input_tmp;
            wit_tmp++;
            wit_tmp++;    //Skip the bias
        }
        num_output = *itlayer;
        itdf = itdf_tmp;
        wit = wit_tmp;
    }
}

void NNParam::computeUnweightedGradients(std::vector<std::vector<float> > &unweighted_grads,
                                         std::set<unsigned int> &used_idxs, std::vector<const FeatureVector *> &fvs,
                                         std::vector<azd_vals_t> &deltasA, std::vector<azd_vals_t> &deltasB,
                                         std::vector<azd_vals_t> &a_values) {

    unsigned int num_trans_from_id = a_values.size();
    unweighted_grads.resize(num_trans_from_id + 1); //+1 for the persistence transition (stored last)
    std::vector<std::vector<float>::iterator> itgrads(num_trans_from_id), itAs(num_trans_from_id), itBs(
            num_trans_from_id), aits(num_trans_from_id);
    for (int idx = 0; idx < num_trans_from_id; idx++) {
        unweighted_grads[idx].clear();
        unweighted_grads[idx].resize(getNumWeightsPerEnergyLevel(), 0.0f);
        itgrads[idx] = unweighted_grads[idx].begin();
        aits[idx] = a_values[idx].begin();
        itAs[idx] = deltasA[idx].begin();
        itBs[idx] = deltasB[idx].begin();
    }

    //Persistence (and normalization) terms
    unweighted_grads[num_trans_from_id].clear();
    unweighted_grads[num_trans_from_id].resize(getNumWeightsPerEnergyLevel(), 0.0f);

    //If there's no transitions from this id except persistence, the unweighted gradients for that persistence transition will all be 0
    if (num_trans_from_id == 0) return;

    std::vector<float>::iterator normit = unweighted_grads[num_trans_from_id].begin();
    unsigned int feature_len = fvs[0]->getTotalLength();

    //Collect the used feature idxs (we'll need these for the first layer normalization)
    int frozen_layer_idx = 0;
    bool froze_layer = hlayer_is_frozen[frozen_layer_idx];
    std::set<unsigned int> tmp_used_idxs;
    std::vector<unsigned int> tmp_used_idxs_vect(feature_len);
    if (!froze_layer){
        for (int idx = 0; idx < num_trans_from_id; idx++) {
            for (auto fit = fvs[idx]->getFeatureBegin(); fit != fvs[idx]->getFeatureEnd(); ++fit) {
                //tmp_used_idxs.insert(*fit);
                tmp_used_idxs_vect[*fit] ++;
                for (int hnode = 0; hnode < h_layer_num_nodes[0]; hnode++)
                    used_idxs.insert(hnode * feature_len + *fit);
            }
        }
    }

    //First layer (only compute gradients for indices relevant to used features)
    std::vector<int>::iterator itlayer = h_layer_num_nodes.begin();

    for (int hnode = 0; hnode < *itlayer; hnode++) {
        //Compute the unnormalized terms, tracking the normalizers (persistence terms) as we go
        if (!froze_layer){
            for (int idx = 0; idx < num_trans_from_id; idx++) {
                float deltaA = *itAs[idx]++;
                float deltaB = *itBs[idx]++;
                for (auto fit = fvs[idx]->getFeatureBegin(); fit != fvs[idx]->getFeatureEnd(); ++fit) {
                    *(itgrads[idx] + *fit) = deltaA;
                    *(normit + *fit) -= deltaB;
                }
            }
            // Apply the normalizers
            // (Note: we need to normalize for all transitions if any transition used that feature)
            for (int idx = 0; idx < num_trans_from_id; idx++) {
                /*std::set<unsigned int>::iterator sit = tmp_used_idxs.begin();
                for (; sit != tmp_used_idxs.end(); ++sit)
                    *(itgrads[idx] + *sit) += *(normit + *sit);*/
                for(int fv_idx = 0; fv_idx < tmp_used_idxs_vect.size(); ++fv_idx)
                    if(tmp_used_idxs_vect[fv_idx] > 0 )
                        *(itgrads[idx] + fv_idx) += *(normit + fv_idx);
                itgrads[idx] += feature_len;
            }
        }
        // keep update normit regard if we update or not
        normit += feature_len;
    }

    //Subsequent layers
    int num_input = *itlayer++;
    for (; itlayer != h_layer_num_nodes.end(); ++itlayer) {
        frozen_layer_idx += 1;
        froze_layer = hlayer_is_frozen[frozen_layer_idx];
        
        for (int hnode = 0; hnode < *itlayer; hnode++) {
            if (!froze_layer) {
                //Compute the unnormalized terms, tracking the normalizers (persistence terms) as we go
                for (int idx = 0; idx < num_trans_from_id; idx++) {
                    float deltaA = *itAs[idx]++;
                    float deltaB = *itBs[idx]++;
                    *itgrads[idx] = deltaA; //Bias term
                    *normit -= deltaB;
                    for (int input_node = 0; input_node < num_input; ++input_node) {
                        float a_val = *(aits[idx] + input_node);
                        *(itgrads[idx] + input_node + 1) = deltaA * a_val;
                        *(normit + input_node + 1) -= deltaB * a_val;
                    }
                }

                //Apply the normalizers
                for (int idx = 0; idx < num_trans_from_id; idx++) {
                    *itgrads[idx] += *normit; //Bias term
                    for (int input_node = 0; input_node < num_input; ++input_node)
                        *(itgrads[idx] + input_node + 1) += *(normit + input_node + 1);
                    itgrads[idx] += (num_input + 1);
                }
            }

            normit += (num_input + 1);
        }

        for (int idx = 0; idx < num_trans_from_id; idx++) 
            aits[idx] += num_input;
        num_input = *itlayer;
    }

}

void NNParam::getBiasIndexes(std::vector<unsigned int> &bias_indexes) {

    bias_indexes.resize(total_nodes);
    unsigned int idx = 0, count = 0;
    auto itlayer = h_layer_num_nodes.begin();
    // Adding bias index for input layer
    for (int hnode = 0; hnode < h_layer_num_nodes[0]; hnode++, count++) {
        bias_indexes[count] = idx;
        idx += expected_num_input_features;
    }

    itlayer ++;
    // Adding bias index for hidden layers
    for (; itlayer != h_layer_num_nodes.end(); ++itlayer) {
        for (int hnode = 0; hnode < *itlayer; hnode++, count++) {
            bias_indexes[count] = idx;
            idx += (*(itlayer - 1) + 1);
        }
    }
}

void NNParam::rollDropouts() {
    if (hlayer_is_dropped.empty())
        hlayer_is_dropped.resize(total_nodes);
    // set everything to false
    std::fill(hlayer_is_dropped.begin(),hlayer_is_dropped.end(), false);

    //adding dropouts in the all layer with the same prob
    int neuron_idx = 0;
    for (int hlayer_idx = 0; hlayer_idx < h_layer_num_nodes.size(); ++hlayer_idx) {
        // P(b|p) = p if b == true
        // P(b|p) = 1-p if b == false
        float drop_ratio = hlayer_dropout_probs[hlayer_idx];// + iter * delta;

        std::vector<int> indice;
        for (int h_node_idx = 0; h_node_idx < h_layer_num_nodes[hlayer_idx]; ++h_node_idx){
            indice.push_back(neuron_idx);
            neuron_idx ++;
        }

        auto expected_deactive_count = (unsigned int)std::floor(drop_ratio * h_layer_num_nodes[hlayer_idx]);
        std::shuffle(indice.begin(),indice.end(),util_rng);
        indice.resize(expected_deactive_count);

        for (const auto & idx: indice)
            hlayer_is_dropped[idx] = true;
    }
}